{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/","title":"CUTLASS: GEMM Kernel by CUTE","text":""},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#1-cute","title":"1. CuTe \u57fa\u7840\u7ec4\u4ef6","text":""},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#11-tensor-layout","title":"1.1 Tensor \u548c Layout","text":"<p>Tensor \u4e2d\u7684\u5f20\u91cf\u5728\u5185\u5b58\u7684\u5b58\u50a8\u7ed3\u6784\u5c31\u662f\u4e00\u79cd Layout\uff0c\u5b83\u5305\u62ec\u4e86 Shape \u548c Stride \u4e24\u4e2a\u90e8\u5206\u3002CuTe \u4e2d\u7684\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u5f0f\u6765\u521b\u5efa</p> <pre><code>Tensor mA = make_tensor(make_gemm_ptr(Aptr),\n                                                make_shape(Int&lt;3&gt;{}, Int&lt;4&gt;{}),\n                                                make_stride(Int&lt;1&gt;{}, Int&lt;3&gt;{}));\n</code></pre>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#12-tiling-api","title":"1.2 Tiling API","text":"<p>\u5728\u5927\u89c4\u6a21\u7684\u77e9\u9635\u8fd0\u7b97\u4e2d\uff0c\u9700\u8981\u5c06\u77e9\u9635\u8fdb\u884c\u5206\u5757\u5904\u7406\uff0c\u4e5f\u5c31\u662f tiling\u3002\u5728 CuTe \u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>local_tile</code> \u6765\u5b9e\u73b0\u5bf9 Tensor \u7684\u5206\u5757\u3002</p> <ul> <li>\u7b2c 2 \u4e2a\u53c2\u6570\u8868\u793a\u5207\u5206\u5757\u7684 shape</li> <li>\u7b2c 3 \u4e2a\u53c2\u6570\u8868\u793a\u5757\u7684\u7d22\u5f15</li> </ul> <pre><code>Tensor gA = local_tile(mA,\n                                             make_shape(Int&lt;kTileM&gt;{}, Int&lt;kTileK&gt;{}), // tile shape\n                                             make_coord(2, 2));\n</code></pre> <p></p> <p>Note: <code>Int&lt;N&gt;{}</code> \u7684\u4f5c\u7528\u662f\u5c06\u4e00\u4e2a \u6570\u503c \u8f6c\u53d8\u6210\u4e00\u4e2a \u6570\u636e\u7c7b\u578b\uff0c\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f  1. \u53ef\u4ee5\u5c06\u5927\u90e8\u5206\u7684\u6570\u636e\u4ece\u8fd0\u884c\u671f\u642c\u5230\u7f16\u8bd1\u671f\u5b9e\u73b0  2. CuTe \u53ef\u4ee5\u901a\u8fc7\u4f20\u5165\u7684\u4e0d\u540c\u6570\u636e\u7c7b\u578b\uff0c\u6bd4\u5982 <code>Int&lt;64&gt;{}</code> \u548c <code>Int&lt;128&gt;{}</code> \uff0c\u6765\u5b9e\u73b0\u6a21\u677f\u7c7b\u7684\u4e0d\u540c\u5b9e\u4f8b\u5316\u3002 </p> <p>\u4e0a\u9762\u7684\u65b9\u5f0f\u5bf9\u77e9\u9635 A/B/C \u5206\u5757\u9700\u8981\u8bbe\u8ba1\u4e0d\u540c\u7684 tile shape\uff0c\u6bd4\u5982\u4e0b\u9762\u8fd9\u6837</p> <pre><code>Tensor gA = local_tile(mA,\n                                             make_shape(Int&lt;kTileM&gt;{}, Int&lt;kTileK&gt;{}), // tile shape\n                                             make_coord(2, 2));\nTensor gB = local_tile(mB,\n                                             make_shape(Int&lt;kTileK&gt;{}, Int&lt;kTileN&gt;{}), // tile shape\n                                             make_coord(2, 2));\nTensor gC = local_tile(mC,\n                                             make_shape(Int&lt;kTileM&gt;{}, Int&lt;kTileN&gt;{}), // tile shape\n                                             make_coord(2, 2));\n</code></pre> <p>\u9664\u4e86\u4e0a\u9762\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u7528\u4e00\u4e2a\u9ad8\u7ef4\u7684 <code>tiler</code>\uff0c\u5e76\u4f20\u5165 <code>Step</code> \u5728\u6307\u5b9a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5206\u5757\uff0c\u8fd9\u6837\u5206\u5757\u5904\u7406\u53ef\u4ee5\u590d\u7528\u540c\u4e00\u4e2a <code>tiler</code> \u548c <code>coord</code></p> <pre><code>auto tiler = make_tile(Int&lt;kTileM&gt;{}, Int&lt;kTileN&gt;{}, Int&lt;kTileK&gt;{});\nauto coord = make_coord(0, 0, 0);\n\nTensor gA = local_tile(mA, tiler, coord, Step&lt;_1, X, _1&gt;{});\n</code></pre> <p>Note:\u00a0<code>make_tile</code>\u00a0\u548c\u00a0<code>make_coord</code>\uff0c\u5305\u62ec\u4e0a\u9762\u7684\u00a0<code>make_shape</code>\u00a0\u548c\u00a0<code>make_stride</code>\uff0c\u6700\u7ec8\u8fd4\u56de\u7684\u90fd\u662f\u4e00\u4e2a\u00a0<code>cute::tuple</code>\u00a0\u7c7b\u578b\u7684\u503c\uff0c\u800c\u00a0<code>Tile</code>\u3001<code>Coord</code>\u3001<code>Shape</code>\u3001<code>Stride</code>\u3001<code>Step</code>\u00a0\u7c7b\u90fd\u662f\u00a0<code>cute::tuple</code>\u00a0\u7684\u522b\u540d\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u4f7f\u7528\u5b83\u4eec\u3002 </p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#13-mma-api","title":"1.3 MMA API","text":"<p>CuTe \u4e2d\u7684 <code>MMA_Atom</code> \u5bf9\u8c61\u5bf9\u5e94\u4e00\u4e2a\u7279\u5b9a\u7684 mma \u6307\u4ee4\uff0c\u4f8b\u5982\u6211\u4eec\u9700\u8981\u5b8c\u6210\u7684 16 \\times 16 \\times 8 \u7684 MMA \u8fd0\u7b97\uff0c\u4e14\u6240\u6709\u7684\u6570\u503c\u7cbe\u5ea6\u5747\u4e3a FP16\uff0c\u90a3\u4e48\u9996\u5148\u9700\u8981\u521b\u5efa\u4e00\u4e2a <code>MMA_op</code></p> <pre><code>using MMA_op = SM80_16x8x8_F16F16F16F16_TN;\n</code></pre> <p>\u5176\u5bf9\u5e94\u7684 mma \u6307\u4ee4\u5982\u4e0b\uff1a</p> <pre><code>mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16\n  {%Rd0, %Rd1},\n  {%Ra0, %Ra1},\n  {%Rb0},\n  {%Rc0, %Rc1};\n</code></pre> <p>\u4e00\u4e2a mma \u6307\u4ee4\u9700\u8981\u4e00\u4e2a warp\uff0832\u4e2a\u7ebf\u7a0b\uff09 \u534f\u4f5c\u5b8c\u6210\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u9700\u8981\u4ece A/B/C \u77e9\u9635\u4e2d\u83b7\u53d6\u6307\u5b9a\u4f4d\u7f6e\u4e0a\u7684\u5143\u7d20\uff0c\u5e76\u5b58\u5165\u5bc4\u5b58\u5668\u4e2d\uff0c\u518d\u5c06\u5bc4\u5b58\u5668\u5582\u7ed9 mma \u6307\u4ee4\u3002\u6bd4\u5982\u8ba1\u7b97 16 \\times 16 \\times 8 \u77e9\u9635\u4e58\u6cd5\u7684\u65f6\u5019\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u9700\u8981 4 \u4e2a\u77e9\u9635 A \u5143\u7d20\u30014 \u4e2a\u77e9\u9635 B \u5143\u7d20\u30018 \u4e2a\u77e9\u9635 C \u5143\u7d20\u3002\u4e0b\u56fe\u663e\u793a\u4e86\u77e9\u9635\u5143\u7d20\u4e0e\u6bcf\u4e2a\u5bc4\u5b58\u5668\u4e2d\u5bc4\u5b58\u5668\u7684\u6620\u5c04\u5173\u7cfb\u3002</p> <p></p> <p>\u53ef\u4ee5\u53d1\u73b0\u7684\u662f\uff0c\u5982\u679c\u4f7f\u7528\u624b\u52a8\u7684\u65b9\u6cd5\u5c06\u77e9\u9635\u5143\u7d20\u6620\u5c04\u5230\u5bf9\u5e94\u7684\u7ebf\u7a0b\u5bc4\u5b58\u5668\u4e0a\uff0c\u662f\u975e\u5e38\u56f0\u96be\u7684\u3002\u800c CuTe \u5e2e\u52a9\u6211\u4eec\u505a\u5230\u4e86\u8fd9\u70b9\uff0c\u5728 Layout Algebra \u7684\u52a0\u6301\u4e0b\uff0cCuTe \u63d0\u4f9b\u7684 MMA API \u5e2e\u52a9\u6211\u4eec\u5efa\u7acb\u4e86\u4e0a\u8ff0\u590d\u6742\u7684\u6620\u5c04\u5173\u7cfb\u3002</p> <p>\u6211\u4eec\u53ea\u9700\u8981\u5c06\u6b63\u786e\u7684 <code>MMA_op</code> \u4f20\u9012\u7ed9 <code>make_tiled_mma</code> \u51fd\u6570\uff0c\u83b7\u53d6\u5230 <code>TiledMMA</code> \u5bf9\u8c61\uff0c\u800c\u8fd9\u4e00\u5bf9\u8c61\u53ef\u4ee5\u5e2e\u52a9\u6bcf\u4e2a\u7ebf\u7a0b\u7d22\u5f15\u5230\u6b63\u786e\u7684\u77e9\u9635\u5143\u7d20\u3002</p> <pre><code>using TiledMMA = decltype(make_tiled_mma(MMA_op{}));\n</code></pre> <p>Note: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u9762 <code>make_tiled_mma</code> \u53ea\u63a5\u6536\u4e86 <code>MMA_op</code> 1 \u4e2a\u53c2\u6570\uff0c\u800c\u5b9e\u9645\u4e0a\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u63a5\u6536 3 \u4e2a\u53c2\u6570\u3002\u8fd9\u8fb9\u53ea\u5199\u4e86 1 \u4e2a\u53c2\u6570\u7684\u539f\u56e0\u662f\uff0c\u6bcf\u4e2a block \u4e2d\u53ea\u6709 1 \u4e2a warp\uff0c\u5e76\u4e14\u6bcf\u4e2a warp \u53ea\u8d1f\u8d23\u8fdb\u884c 1 mma \u6307\u4ee4\u8ba1\u7b97\u3002\u5bf9 <code>make_tiled_mma</code> \u5176\u4ed6\u53c2\u6570\u7684\u4ecb\u7ecd\u4f1a\u5728\u7b2c 3 \u8282\u505a\u51fa\u4ecb\u7ecd\u3002 </p> <p><code>TiledMMA</code> \u7684\u5b9e\u4f8b\u5316\u662f\u5728\u6bcf\u4e2a kernel \u51fd\u6570\u5f53\u4e2d\u6267\u884c\u7684\uff0c\u5e76\u901a\u8fc7\u00a0<code>get_slice</code>\u00a0\u62ff\u5230\u5bf9\u5e94\u7ebf\u7a0b\u7684 tiler\uff08\u5373 CuTe \u7684 <code>ThrMMA</code> \u5b9e\u4f8b\uff09\u3002\u8c03\u7528\u8fd9\u4e2a tiler \u7684\u00a0<code>partition_A</code>\u00a0\u65b9\u6cd5\uff0c\u5c31\u62ff\u5230\u4e86\u8be5\u7ebf\u7a0b\u5b8c\u6210 MMA \u8ba1\u7b97\u6240\u9700\u7684 A \u77e9\u9635\u5143\u7d20\u7684 Tensor \u8868\u793a\uff0c\u8fd9\u4e2a Tensor\u00a0\u8868\u793a\u4e86 global memory \u4e0a A \u77e9\u9635\u5bf9\u5e94\u5230\u8fd9\u4e2a\u7ebf\u7a0b\u7684\u5206\u7247\u3002\u76f8\u5e94\u8fd8\u6709\u00a0<code>partition_B</code>\u3001<code>partition_C</code>\u00a0\u65b9\u6cd5\uff0c\u5b83\u4eec\u7684\u4f5c\u7528\u7c7b\u4f3c\u3002</p> <pre><code>TiledMMA tiled_mma;\nThrMMA thr_mma = tiled_mma.get_slice(tid);\n\nTensor tCgA = thr_mma.partition_A(gA);  // (MMA, MMA_M, MMA_K)\n// MMA     1 \u4e2a\u539f\u5b50\u64cd\u4f5c\u9700\u8981\u7684\u6570\u636e\n// MMA_M   M \u65b9\u5411\u91cd\u590d\u7684\u6b21\u6570\n// MMA_K   K \u65b9\u5411\u91cd\u590d\u7684\u6b21\u6570\n</code></pre> <p><code>ThrMMA</code> \u8fd8\u6709\u4e00\u4e2a <code>partition_fragment_A</code> \u65b9\u6cd5\uff0c\u5b83\u8fd4\u56de\u7684 Tensor \u7684 shape \u548c partition_A \u76f8\u540c\uff0c\u4f46\u662f\u8fd9\u4e2a Tensor \u4e0d\u8868\u793a <code>global memory</code> \u7684\u6570\u636e\uff0c\u800c\u662f\u8868\u793a\u7ebf\u7a0b\u5185\u7684\u4e00\u7ec4\u8fde\u7eed\u7684\u5bc4\u5b58\u5668\u3002</p> <pre><code>Tensor tCrA = thr_mma.partition_fragment_A(gA);  // (MMA, MMA_M, MMA_K)\n</code></pre>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#14-copy-api-gemm-api","title":"1.4 Copy API \u4e0e GEMM API","text":"\ud83d\udca1  \u672c\u6587\u4e2d Copy API \u7684\u4ecb\u7ecd\u662f\u4e3a\u4e86\u5b9e\u73b0\u6700\u7b80\u5355\u7684 GEMM\uff0c\u56e0\u6b64\u6bd4\u8f83\u7b80\u5355\u3002   <p>\u53ef\u4ee5\u7528 CuTe \u63d0\u4f9b\u7684 Copy API \u5b8c\u6210\u6570\u636e\u7684\u62f7\u8d1d\u3002\u4f8b\u5982\u4e0b\u9762\u7684\u4ee3\u7801\u5b8c\u6210\u4e86\u6570\u636e\u4ece global memory \u5230\u5bc4\u5b58\u5668\u7684\u62f7\u8d1d\uff1a</p> <pre><code>auto copy_atom = AutoVectorizingCopy{};\ncopy(copy_atom, tCgA, tCrA);\n</code></pre> <p>\u6570\u636e\u5c31\u7eea\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u8c03\u7528 CuTe GEMM API \u8fdb\u884c mma \u7684\u8ba1\u7b97\uff1a</p> <pre><code>gemm(tiled_mma, tCrD, tCrA, tCrB, tCrC);\n</code></pre> <p>\u968f\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u7ed3\u679c\u5199\u56de global memory\uff1a</p> <pre><code>copy(copy_atom, tCrD, tCgD);\n</code></pre>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#2-minimal-gemm-kernel","title":"2. Minimal GEMM Kernel","text":""},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#21","title":"2.1 \u4ee3\u7801\u5b9e\u73b0","text":"<p>\u672c\u8282\u4e2d\u9700\u8981\u89e3\u51b3\u7684\u95ee\u9898\u6bd4\u8f83\u7b80\u5355\uff0c\u56e0\u6b64\u4ee3\u7801\u5b9e\u73b0\u4e5f\u662f\u975e\u5e38\u7b80\u5355\u3002\u4ece\u4e0b\u9762\u7684\u8868\u683c\u53ef\u4ee5\u770b\u51fa\uff0c\u6211\u4eec\u4f7f\u7528 mma \u6307\u4ee4 <code>mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16</code> \uff0c\u5e76\u4e14\u9700\u8981\u6211\u4eec\u8ba1\u7b97\u7684\u77e9\u9635\u89c4\u6a21\u4e5f\u662f 16 \\times 16 \\times 8 \uff0c\u56e0\u6b64\u4e0d\u9700\u8981 tiling\u3002</p> \u95ee\u9898\u89c4\u6a21 (16, 8, 8) \u7b97\u5b50\u7cbe\u5ea6 fp16 = fp16 * fp16 + fp16 Grid shape (1, 1, 1) Block shape (32, 1, 1) Block tile shape (16, 8, 8) Tiled MMA shape (16, 8, 8) MMA Atom shape (16, 8, 8) <p>\u5177\u4f53\u7684\u4ee3\u7801\u5b9e\u73b0\u4f4d\u4e8e\u8fd9\u91cc\u3002</p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#22","title":"2.2 \u6027\u80fd\u5206\u6790","text":"<p>TODO</p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#3-gemm-kernel","title":"3. \u6df7\u5408\u7cbe\u5ea6 GEMM Kernel","text":"<p>TODO</p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#4-cute-tiling","title":"4. CUTE \u4e0b\u7684\u4e09\u7ea7 Tiling \u6a21\u578b","text":"<p>\u5728 2.1 \u7684\u8868\u683c\u4e2d\u5df2\u7ecf\u63d0\u5230\u8fc7\uff0cCUTE \u5728\u5b9e\u73b0 GEMM \u7684\u65f6\u5019\uff0c\u8fdb\u884c\u4e86\u4e09\u7ea7 Tiling\uff0c\u5305\u62ec MMA Atom shape\u3001Tiled MMA shape \u548c Block Tile shape\u3002</p> <ul> <li>MMA Atom shape: \u5bf9\u5e95\u5c42 PTX mma \u6307\u4ee4\u7684\u5c01\u88c5</li> <li>Tiled MMA shape: \u7531 MMA Atom \u5728 MNK \u7ef4\u5ea6\u7684 \u6392\u5e03\u65b9\u5f0f \u548c \u6267\u884c\u6b21\u6570 \u5171\u540c\u7ec4\u6210</li> <li>Block Tile shape: \u5728\u4e00\u4e2a block \u5f53\u4e2d\uff0c\u4e5f\u5c31\u662f\u4e00\u4e2a kernel \u51fd\u6570\u5185\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u65b9\u5f0f\u4e32\u884c\u6267\u884c Tiled MMA\uff0c\u5171\u540c\u7ec4\u6210\u4e86\u4e00\u4e2a Block \u6765\u8d1f\u8d23\u7684 tile</li> </ul> <p></p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#41-tiled-mma","title":"4.1 Tiled MMA","text":"<p>\u5728\u672c\u5c0f\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u9996\u5148\u6269\u5c55 MMA Atom \u6765\u83b7\u5f97\u66f4\u5927\u5c3a\u5bf8\u7684 Tiled MMA\uff0c\u800c\u8fd9\u4e00\u6b65\u9aa4\u53ef\u4ee5\u901a\u8fc7\u51fd\u6570 <code>make_tiled_mma</code> \u5b9e\u73b0\u3002</p> <p>\u5982\u4e0a\u6240\u8ff0\uff0cTiled MMA \u662f\u7531 MMA Atom \u5728 MNK \u7ef4\u5ea6\u6539\u53d8 \u6392\u5e03\u65b9\u5f0f \u548c \u6267\u884c\u6b21\u6570 \u5f97\u5230\u7684\u3002\u6392\u5e03\u65b9\u5f0f\u7684\u6539\u53d8\u5176\u5b9e\u5c31\u662f\u589e\u52a0\\/\u51cf\u5c11 warp \u6570\u91cf\uff0c\u4e5f\u5c31\u662f\u589e\u52a0\\/\u5e76\u53d1\u6570\u91cf\uff0c\u800c\u6267\u884c\u6b21\u6570\u7684\u6539\u53d8\u5176\u5b9e\u5c31\u662f\u589e\u52a0\\/\u51cf\u5c11\u5355\u4e2a warp \u6267\u884c MMA Atom \u7684\u6b21\u6570\uff0c\u4e5f\u5c31\u662f\u589e\u52a0\\/\u4e32\u884c\u6267\u884c\u6570\u91cf\u3002</p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#make_tiled_mma-api","title":"make_tiled_mma API","text":"<p>\u5728 1.2 \u5c0f\u8282\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5f88\u7b80\u5355\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u8fd9\u4e2a\u77e9\u9635 shape \u548c MMA Atom shape \u662f\u4e00\u6837\u7684\uff0c\u56e0\u6b64 <code>make_tiled_mma</code> \u7684\u4f7f\u7528\u975e\u5e38\u7b80\u5355\uff0c\u8868\u793a\u83b7\u5f97\u7684 Tiled MMA shape \u548c MMA Atom shape \u662f\u4e00\u6837\u7684</p> <pre><code>using TiledMMA = decltype(make_tiled_mma(MMA_op{}));\n</code></pre> <p>\u800c\u5b9e\u8d28\u4e0a\uff0c <code>make_tiled_mma</code> \u9664\u4e86 <code>MMA_op</code> \u8fd8\u53ef\u4ee5\u63a5\u53d7\u4e24\u4e2a\u53c2\u6570 <code>MMAThrLayout</code> \u548c <code>MMATileLayout</code> </p> <p></p> <ul> <li><code>MMA_op</code> \u901a\u5e38\u5bf9\u5e94\u4e00\u4e2a\u539f\u5b50\u6307\u4ee4\uff0c\u4e0e <code>MMA_traits</code> <code>MMA_atom</code> \u4e00\u4e00\u5bf9\u5e94\uff0c\u5c01\u88c5\u4e86\u6307\u4ee4\u5bf9\u5e94\u7684\u6570\u636e\u5904\u7406\u5f62\u72b6\u3001\u6570\u636e\u7c7b\u578b\u3001\u7ebf\u7a0b\u6570\u91cf\u7b49</li> <li><code>MMAThrLayout</code> cute\u5f53\u4e2d\u7684 <code>layout</code> \u5bf9\u8c61\uff0c\u89c4\u5b9a\u4e86\u5728 m n k \u65b9\u5411\u539f\u5b50\u5757(Atom)\u7684\u5806\u53e0\u6570\u91cf\uff0c\u901a\u8fc7\u8fd9\u4e2a\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230\u5904\u7406\u8be5 tile \u7684\u7ebf\u7a0b\u603b\u6570\u91cf</li> <li><code>MMATileLayout</code> cute\u5f53\u4e2d\u7684 <code>layout</code> \u5bf9\u8c61\uff0c\u8868\u660e\u4e86\u5f85\u5904\u7406 tile \u5728 m n k \u65b9\u5411\u4e0a\u7684 shape</li> </ul>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#_1","title":"\u4ee3\u7801\u5b9e\u73b0","text":"<p>\u76f8\u6bd4\u4e8e Minimal GEMM Kernel\uff0c\u672c\u5c0f\u8282\u4e3b\u8981\u662f\u6269\u5c55 Tiled MMA shape\uff0c\u5f53\u7136\u4e5f\u6269\u5c55\u5f85\u5904\u7406\u77e9\u9635\u7684\u5927\u5c0f\uff0c\u4f46\u662f\u4fdd\u6301\u4e86 Block Tile shape \u548c Tiled MMA shape \u662f\u4e00\u81f4\u7684\u3002</p> \u95ee\u9898\u89c4\u6a21 (32, 32, 16) \u7b97\u5b50\u7cbe\u5ea6 bf16 = bf16 * bf16 + fp32 Grid shape (1, 1, 1) Block shape (256, 1, 1) Block tile shape (32, 32, 16) Tiled MMA shape (32, 32, 16) MMA Atom shape (16, 8, 8) <p>\u8fd9\u5757\u7684\u4fee\u6539\u975e\u5e38\u7b80\u5355\uff0c\u76f8\u6bd4\u4e8e Minimal GEMM Kernel\uff0c\u53ea\u9700\u8981\u4fee\u6539 <code>make_tiled_mma</code> \u6765\u83b7\u5f97\u65b0\u7684 <code>TiledMMA</code> \u5c31\u597d\u3002</p> <pre><code>using namespace cute;\n\nusing MMA_op = SM80_16x8x8_F32BF16BF16F32_TN;\nusing MMA_traits = MMA_Traits&lt;MMA_op&gt;;\nusing MMA_atom = MMA_Atom&lt;MMA_traits&gt;;\nusing MMA_shape = MMA_traits::Shape_MNK;\n\nstatic constexpr int kMmaThrExpandM = 2;\nstatic constexpr int kMmaThrExpandN = 4;\nstatic constexpr int kMmaThrExpandK = 1;\n\nstatic constexpr int kMmaValExpandM = 1;\nstatic constexpr int kMmaValExpandN = 1;\nstatic constexpr int kMmaValExpandK = 2;\n\nstatic constexpr int kMmaTileM = kMmaThrExpandM * kMmaValExpandM * get&lt;0&gt;(MMA_shape{});\nstatic constexpr int kMmaTileN = kMmaThrExpandN * kMmaValExpandN * get&lt;1&gt;(MMA_shape{});\nstatic constexpr int kMmaTileK = kMmaThrExpandK * kMmaValExpandK * get&lt;2&gt;(MMA_shape{});\n\nusing MMAThrLayout = decltype(make_layout(make_shape(Int&lt;kMmaThrExpandM&gt;{},\n                                                     Int&lt;kMmaThrExpandN&gt;{},\n                                                     Int&lt;kMmaThrExpandK&gt;{})));\nusing MMATileLayout = Tile&lt;Int&lt;kMmaTileM&gt;, Int&lt;kMmaTileN&gt;, Int&lt;kMmaTileK&gt;&gt;;\nusing TiledMMA = decltype(make_tiled_mma(MMA_op{}, MMAThrLayout{}, MMATileLayout{}));\n</code></pre> <p>\u5b8c\u6574\u7684\u4ee3\u7801\u5728\u8fd9\u91cc</p>"},{"location":"CUDA/CUTLASS%20GEMM%20Kernel%20by%20CUTE/#13-mma-api_1","title":"1.3 MMA API","text":"<p><code>MMA_Atom</code> \u4ee3\u8868\u4e86\u786c\u4ef6\uff08\u901a\u5e38\u662f Tensor Core\uff09\u80fd\u591f\u6267\u884c\u7684\u6700\u5c0f\u3001\u4e0d\u53ef\u5206\u5272\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u5355\u5143\u3002 <code>MMA_Atom</code> \u662f\u7528\u6765\u63cf\u8ff0 <code>mma.sync</code> \u6307\u4ee4\u7684\u8f6f\u4ef6\u5bf9\u8c61\uff0c\u5b83\u5c01\u88c5\u4e86\uff1a</p> <ul> <li>\u6307\u4ee4\u5f62\u72b6\uff08shape\uff09: \u4f8b\u5982 m16_n8_k16</li> <li>\u6570\u636e\u7c7b\u578b: \u4f8b\u5982 A \u662f fp16\uff0cC \u662f fp32</li> <li>\u7ebf\u7a0b\u5e03\u5c40: \u4e00\u4e2a warp \u4e2d\u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u54ea\u4e9b\u6570\u636e\u3002\u5305\u62ec 32 \u4e2a\u7ebf\u7a0b\u5982\u4f55\u6301\u6709 A B C \u77e9\u9635\u7684\u6570\u636e\u7247\u6bb5</li> </ul> <p>\u5bf9\u4e8e Tensor Core \u7684\u4e00\u6761 <code>mma.sync</code> \u6307\u4ee4\u6765\u8bf4\uff0c\u9700\u8981\u5c06\u4e00\u4e2a warp \u8d1f\u8d23\u7684\u77e9\u9635\u6570\u636e\u5207\u5206\u5230\u5bf9\u5e94\u7ebf\u7a0b\u7684\u5bf9\u5e94\u5bc4\u5b58\u5668\u4e0a\uff0c\u7136\u540e\u518d\u8fd0\u884c\u8be5\u6307\u4ee4\u3002\u800c <code>MMA_Atom</code> \u76f8\u5f53\u4e8e\u5df2\u7ecf\u5c06\u6570\u636e\u5207\u5206\u7684\u64cd\u4f5c\u5c01\u88c5\u8d77\u6765\u4e86\u3002</p> <pre><code>using MMA_op = MMA_Atom&lt;SM80_16x8x16_F32F16F16F32_TN&gt;;\n</code></pre> <p>\u786c\u4ef6\u7684 Tensor Core \u6307\u4ee4\u53ef\u4ee5\u5904\u7406\u7684\u77e9\u9635\u5f88\u5c0f\uff0c\u56e0\u6b64\u9700\u8981\u591a\u4e2a Warp \u7684\u5806\u53e0\u6765\u5904\u7406\u66f4\u5927\u7ef4\u5ea6\u7684\u77e9\u9635</p> <pre><code>// M = 16 * 2 = 32\n// N =  8 * 2 = 16\n// K = 16 * 1 = 16\n// TiledMMA \u53ef\u4ee5\u5904\u7406 32*16*16 \u7684\u77e9\u9635\n\nusing TiledMMA = decltype(make_tiled_mma(\n                                    MMA_op{},\n                                    Layout&lt;Shape&lt;2,2,1&gt;&gt;{}\n                                 )):\n</code></pre> <p>\u4e0a\u9762\u7684\u4ee3\u7801\u5b9a\u4e49\u4e86 <code>MMA_op</code> <code>TiledMMA</code> \u901a\u8fc7\u4e0b\u9762\u7684\u6b65\u9aa4\u53ef\u4ee5\u8ba9\u6bcf\u4e2a\u7ebf\u7a0b\u5f88\u65b9\u4fbf\u5730\u5bfb\u5740\u5230\u81ea\u5df1\u9700\u8981\u8d1f\u8d23\u7684\u6570\u636e</p> <pre><code>// 1. \u5728 kernel \u5185\u90e8\u5b9e\u4f8b\u5316\nTiledMMA tiled_mma;\n\n// 2. \u83b7\u53d6\u5f53\u524d\u7ebf\u7a0b\u7684\u4efb\u52a1\nauto thr_mma = tiled_mma.get_slice(thread_idx);\n\n// 3. \u6570\u636e\u5207\u7247\nauto tAsA = thr_mma.partition_A(sA);  // \u6bcf\u5757\u7ebf\u7a0b\u5e94\u8be5\u53bb\u8bfb\u90a3\u4e00\u5757\u5730\u5740\nauto tBrA = thr_mma.partition_fragment_A(sA);  // \u6bcf\u4e2a\u7ebf\u7a0b\u7528\u6765\u5b58\u653e\u6570\u636e\u7684\u5bc4\u5b58\u5668\n</code></pre>"},{"location":"CUDA/Tensor%20Cores/","title":"Tensor Cores","text":""},{"location":"CUDA/Tensor%20Cores/#_1","title":"\u6982\u8ff0","text":"<p>Tensor Core \u662f Nvidia \u81ea Volta \u67b6\u6784\uff0c\u5373 CUDA 9.0 \u7248\u672c\u5f15\u5165\u7684\u4e13\u95e8\u7528\u4e8e\u505a\u77e9\u9635\u4e58\u52a0\u7684\u8fd0\u7b97\u5355\u5143\u3002\u8fd9\u91cc\u4e3e\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\uff0cV100 \u62e5\u6709 640 \u4e2a Tensor Core\uff0c\u6bcf\u5757 SM \u4e0a\u62e5\u6709 8 \u4e2a Tensor Core\uff0c\u56e0\u6b64\u53ef\u4ee5\u63d0\u4f9b 125 TFLOPS \u7684\u7b97\u529b\u3002</p> <p>\u6bcf\u4e2a Tensor Core \u5728\u6bcf\u4e2a\u65f6\u949f\u5468\u671f\u5185\u53ef\u4ee5\u5904\u7406 4*4*4 \u7684\u77e9\u9635\u4e58\u52a0\u8fd0\u7b97\uff0c\u5373 64 \u6b21 FMA\u3002\u5982\u4e0b\u56fe\u6240\u8868\u793a\u7684\u8fd0\u7b97 D=A*B+C\uff0c\u5176\u4e2d A \u548c B \u662f FP16 \u7c7b\u578b\u7684\u77e9\u9635\uff0c\u800c\u7d2f\u52a0\u77e9\u9635 C \u548c D \u4e3a FP16 \u6216\u8005 FP32 \u7c7b\u578b\u3002</p> <p></p> <p></p> <p>\u4e00\u4e2a\u5b8c\u6574\u7684 Warp \u4f1a\u5e76\u53d1\u4f7f\u7528\u591a\u4e2a TensorCore\uff0cWarp \u5185\u7684\u7ebf\u7a0b\u534f\u4f5c\u4f1a\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5927\u7684 16*16*16 \u7684\u77e9\u9635\u8fd0\u7b97\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 C++ WMMA API \u6765\u5b9e\u73b0\u8fd9\u4e9b Warp \u7ea7\u77e9\u9635\u8fd0\u7b97\u3002</p>"},{"location":"CUDA/Tensor%20Cores/#cublas-tensor-core","title":"\u5728 cuBLAS \u4e2d\u4f7f\u7528 Tensor Core","text":""},{"location":"CUDA/Tensor%20Cores/#_2","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>// First, create a cuBLAS handle:\ncublasStatus_t cublasStat = cublasCreate(&amp;handle);\n\n// Set the math mode to allow cuBLAS to use Tensor Cores:\ncublasStat = cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);\n\n// Allocate and initialize your matrices (only the A matrix is shown):\nsize_t matrixSizeA = (size_t)rowsA * colsA;\nT_ELEM_IN **devPtrA = 0;\n\ncudaMalloc((void**)&amp;devPtrA[0], matrixSizeA * sizeof(devPtrA[0][0]));\nT_ELEM_IN A  = (T_ELEM_IN *)malloc(matrixSizeA * sizeof(A[0]));\n\nmemset( A, 0xFF, matrixSizeA* sizeof(A[0]));\nstatus1 = cublasSetMatrix(rowsA, colsA, sizeof(A[0]), A, rowsA, devPtrA[i], rowsA);\n\n// ... allocate and initialize B and C matrices (not shown) ...\n\n// Invoke the GEMM, ensuring k, lda, ldb, and ldc are all multiples of 8, \n// and m is a multiple of 4:\ncublasStat = cublasGemmEx(handle, transa, transb, m, n, k, alpha,\n                          A, CUDA_R_16F, lda,\n                          B, CUDA_R_16F, ldb,\n                          beta, C, CUDA_R_16F, ldc, CUDA_R_32F, algo);\n</code></pre> <p>\u4f7f\u7528\u89c4\u5219</p> <ul> <li>\u53ea\u6709 GEMM \u652f\u6301\u4f7f\u7528 Tensor Core</li> <li>\u8ba1\u7b97\u6a21\u5f0f\u5fc5\u987b\u8bbe\u7f6e\u4e3a <code>CUBLAS_TENSOR_OP_MATH</code></li> <li><code>k</code>\u3001<code>lda</code>\u3001<code>ldb</code>\u548c<code>ldc</code>\u5fc5\u987b\u662f 8 \u7684\u500d\u6570\uff1b<code>m</code>\u5fc5\u987b\u662f 4 \u7684\u500d\u6570\u3002Tensor Core \u6570\u5b66\u4f8b\u7a0b\u4ee5 8 \u4e2a\u503c\u4e3a\u4e00\u6b65\u8de8\u8d8a\u8f93\u5165\u6570\u636e\uff0c\u56e0\u6b64\u77e9\u9635\u7684\u7ef4\u5ea6\u5fc5\u987b\u662f 8 \u7684\u500d\u6570</li> <li>\u77e9\u9635\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\u7c7b\u578b\u5fc5\u987b\u662f\u534a\u7cbe\u5ea6\u6216\u5355\u7cbe\u5ea6</li> <li>\u4e0d\u6ee1\u8db3\u4e0a\u8ff0\u89c4\u5219\u7684 GEMM \u5c06\u56de\u9000\u5230\u975e Tensor Core \u5b9e\u73b0</li> </ul>"},{"location":"CUDA/Tensor%20Cores/#cuda-c-tensor-core","title":"\u5728 CUDA C++ \u4e2d\u4f7f\u7528 Tensor Core","text":"<p>CUDA 9.0 \u901a\u8fc7 <code>nvcuda::wmma</code> \u547d\u540d\u7a7a\u95f4\u4e0b\u7684\u4e00\u7ec4\u51fd\u6570\u548c\u7c7b\u578b\uff0c\u63d0\u4f9b\u4e86\u5bf9 Tensor Core \u7684\u652f\u6301\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4ee3\u7801\u793a\u4f8b\u3002</p>"},{"location":"CUDA/Tensor%20Cores/#_3","title":"\u5934\u6587\u4ef6\u548c\u547d\u540d\u7a7a\u95f4","text":"<pre><code>#include &lt;mma.h&gt;\nusing namespace nvcuda;\n</code></pre>"},{"location":"CUDA/Tensor%20Cores/#_4","title":"\u58f0\u660e\u548c\u521d\u59cb\u5316","text":"<p>\u6709\u6548\u7684\u7b56\u7565\u662f\u8ba9\u5355\u4e2a Warp(\u7ebf\u7a0b\u675f) \u8d1f\u8d23\u8f93\u51fa\u77e9\u9635\u4e2d\u7684\u4e00\u4e2a 1616 \u7684\u533a\u57df\uff0c\u901a\u8fc7\u8bbe\u7f6e\u5408\u7406\u7684 Grid \u548c Thread Block\uff0c\u53ef\u4ee5\u5c06 Warp \u6709\u6548\u5730\u5e73\u94fa\u8986\u76d6\u5728\u4e8c\u7ef4\u8f93\u51fa\u77e9\u9635*\u4e0a\u3002</p> <pre><code>// The only dimensions currently supported by WMMA\nconst int WMMA_M = 16;\nconst int WMMA_N = 16;\nconst int WMMA_K = 16;\n\n__global__ void wmma_example(half *a, half *b, float *c, \n                             int M, int N, int K, \n                             float alpha, float beta) \n{\n\n    // Leading dimensions. Packed with no transpositions.\n    int lda = M;\n    int ldb = K;\n    int ldc = M;\n\n    // Tile using a 2D grid\n    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;\n    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n</code></pre> <p>\u5728\u6267\u884c MMA\uff08\u77e9\u9635\u4e58\u52a0\uff09\u64cd\u4f5c\u4e4b\u524d\uff0c\u53c2\u4e0e\u8fd0\u7b97\u7684\u77e9\u9635\u5fc5\u987b\u5148\u88ab\u52a0\u8f7d\u5230 GPU \u7684\u5bc4\u5b58\u5668\u4e2d\u3002\u7531\u4e8e WMMA \u662f Warp \u7ea7\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u5bc4\u5b58\u5668\u5206\u5e03\u5728 Warp \u7684\u5404\u4e2a\u7ebf\u7a0b\u4e2d\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u6301\u6709\u6574\u4f53\u77e9\u9635\u7684\u4e00\u90e8\u5206\u3002</p> <p>\u5728 CUDA \u4e2d\uff0c<code>fragment</code> \u662f\u4e00\u4e2a\u6a21\u677f\u7c7b\u578b\uff0c\u5176\u6a21\u677f\u53c2\u6570\u63cf\u8ff0\u4e86\u4e00\u4e0b\u5185\u5bb9\uff1a</p> <ul> <li>\u8be5\u7247\u6bb5\u6240\u6301\u6709\u7684\u77e9\u9635\uff08<code>wmma::matrix_a</code> <code>wmma::matrix_b</code> <code>wmma::accumulator</code>\uff09</li> <li>WMMA \u64cd\u4f5c\u7684\u77e9\u9635 shape</li> <li>\u77e9\u9635\u6570\u636e\u7c7b\u578b</li> <li>\u5bf9\u4e8e<code>wmma::matrix_a</code> <code>wmma::matrix_b</code> \uff0c\u9700\u8981\u8bf4\u660e\u662f\u884c\u4e3b\u5e8f\u8fd8\u662f\u5217\u4e3b\u5e8f\uff08\u6700\u540e\u4e00\u4e2a\u53c2\u6570\u53ef\u4ee5\u5b9e\u73b0\u77e9\u9635\u7684\u8f6c\u7f6e\u64cd\u4f5c\uff09</li> </ul> <pre><code>// Declare the fragments\nwmma::fragment&lt;wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major&gt; a_frag;\nwmma::fragment&lt;wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major&gt; b_frag;\nwmma::fragment&lt;wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float&gt; acc_frag;\nwmma::fragment&lt;wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float&gt; c_frag;\n\n// fill the accumulator fragment with zeros.\nwmma::fill_fragment(acc_frag, 0.0f);\n</code></pre> <p><code>fragment</code> \u8bf4\u660e</p> <p><code>fragment</code> \u5728\u7269\u7406\u4e0a\u662f\u5206\u5e03\u5b58\u50a8\u5728\u4e00\u4e2a Warp 32 \u4e2a\u7ebf\u7a0b\u7684\u5bc4\u5b58\u5668\u6587\u4ef6\u4e0a\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7 <code>wmma::load_matrix_sync</code> \u6307\u4ee4\uff0c\u5c06\u6570\u636e\u4ece\u663e\u5b58\uff08Global Memory\uff09\u6216\u5171\u4eab\u5185\u5b58\uff08shared memory\uff09\u52a0\u8f7d\u5230\u5bc4\u5b58\u5668\u4e2d\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u786c\u4ef6\u81ea\u52a8\u5206\u53d1\u7684\u3002</p> <ul> <li>\u4e00\u4e2a Warp 32 \u4e2a\u7ebf\u7a0b\u540c\u65f6\u8c03\u7528<code>wmma::load_matrix_sync</code>\u6307\u4ee4\uff0c\u6307\u5411\u7684\u662f\u540c\u4e00\u5757\u6570\u636e\u6e90\u5730\u5740\uff1b</li> <li>GPU \u786c\u4ef6\u8bfb\u53d6\u5185\u5b58\u5757\u4e2d\u7684\u6570\u636e\uff0c\u5e76\u6839\u636e\u4e0d\u900f\u660e\u7684\u89c4\u5219\uff0c\u5c06\u8fd9\u4e9b\u6570\u636e\u62c6\u5206\u5e76\u6295\u5165 32 \u4e2a\u7ebf\u7a0b\u7684\u5bc4\u5b58\u5668\u4e2d\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u83b7\u53d6<code>fragment</code>\u7684\u4e00\u90e8\u5206\u6570\u636e\u3002</li> </ul>"},{"location":"CUDA/Tensor%20Cores/#_5","title":"\u5185\u90e8\u5faa\u73af","text":"<p>\u8be5 GEMM \u7684\u7b56\u7565\u662f\u8ba9\u6bcf\u4e2a Warp \u8ba1\u7b97\u8f93\u51fa\u77e9\u9635\u7684\u4e00\u4e2a Tile\u3002\u4e3a\u6b64\uff0c\u9700\u8981\u6cbf\u7740 A \u77e9\u9635\u7684\u884c\u548c B \u77e9\u9635\u7684\u5217\u8fdb\u884c\u5faa\u73af\uff0c\u6700\u7ec8\u751f\u6210\u4e00\u4e2a M*N \u7684\u8f93\u51fa\u5757\u3002</p> <p>TODO: Leading Dimension\uff08\u4e3b\u7ef4\u5ea6\uff09</p> <pre><code>// Loop over the K-dimension\nfor (int i = 0; i &lt; K; i += WMMA_K) {\n    int aRow = warpM * WMMA_M;\n    int aCol = i;\n    int bRow = i;\n    int bCol = warpN * WMMA_N;\n\n    // Bounds checking\n    if (aRow &lt; M &amp;&amp; aCol &lt; K &amp;&amp; bRow &lt; K &amp;&amp; bCol &lt; N) {\n        // Load the inputs\n        wmma::load_matrix_sync(a_frag, a + aRow + aCol * lda, lda);\n        wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);\n\n        // Perform the matrix multiplication\n        wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n    }\n}\n</code></pre>"},{"location":"CUDA/Tensor%20Cores/#_6","title":"\u53c2\u8003\u6587\u6863","text":"<p>Programming Tensor Cores in CUDA 9</p>"},{"location":"CUDA/%E6%96%87%E6%A1%A3%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86/","title":"\u6587\u6863\u9605\u8bfb\u6574\u7406","text":"\u4e13\u680f\u7cfb\u5217 \u7b80\u4ecb \u6587\u6863\u539f\u94fe\u63a5 \u5b66\u4e60\u6587\u6863/\u4ee3\u7801\u94fe\u63a5 \u72b6\u6001 \u4e13\u680f\uff1aCUTLASS\u7b14\u8bb0\u7cfb\u5217 \u4ece\u96f6\u5b9e\u73b0\u9ad8\u6027\u80fd CUDA \u7b97\u5b50\uff0c\u6df1\u5165\u89e3\u6790 CUTLASS \u5e93\u7684 GEMM \u5f00\u53d1\u4e0e\u4f18\u5316\u6280\u5de7\u3002 Minimal GEMM Kernel \u2705\u5b8c\u6210 \u6df7\u5408\u7cbe\u5ea6 GEMM Kernel Tiled MMA \u2705\u5b8c\u6210 Tiled Copy Block MMA Block Copy \u4e13\u680f\uff1aCUTLASS\u5b9e\u6218 \u3010CUTELASS\u5b9e\u6218\u3011\u4f7f\u7528CUTELASS\u5b9e\u73b0\u9ad8\u6027\u80fd GEMV CUTELASS \u5b9e\u6218 - GEMV \u3010CUTELASS\u5b9e\u6218\u3011LLM Weight-Only Quantization Mixed GEMV \u9ad8\u6027\u80fd\u5b9e\u73b0 \u4e13\u680f\uff1a\u5199\u7ed9\u5927\u5bb6\u770b\u7684 Cute \u6559\u7a0b CUDA GEMM \u7406\u8bba\u6027\u80fd\u5206\u6790\u4e0e kernel \u4f18\u5316 \u4f7f\u7528 MMA \u7b80\u5355\u5165\u95e8 GEMM / A gentle introduction to GEMM using MMA tensor cores CUDA \u7b97\u5b50\u624b\u6495\u4e0e\u9762\u8bd5 FlashInfer Writing Speed-of-Light Flash Attention for 5090 in CUDA C++"}]}